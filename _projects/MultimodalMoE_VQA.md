---
layout: page
title: Multimodal MoE for VQA
description: A LVLM fine-tuned for engineering reasoning and tasks that leverages a Mixture of Experts (MoE) framework for computational efficiency.
img: assets/img/CS224N_Project_Poster.png
importance: 3
category: academic
#related_publications: 
---
<div class="caption">
    <b>Abstract</b>
</div>
Engineers spend significant time searching for multi-modal information across dense technical documents, graphical presentations, and numerical data — a task both time-consuming and prone to errors that has the potential to evolve significantly with the advent of Large Vision Language Models (LVLMs). Current solutions, while general in application, fall short in addressing the unique challenges posed by the engineering field, which demands a blend of precision, speed, and the ability to reason over complex, multi-modal datasets. This project, therefore, seeks to fill this gap by introducing the first LVLM specifically fine-tuned for engineering reasoning and tasks that leverages a Mixture of Experts (MoE) framework for computational efficiency.

<div class="image">
{% include figure.html path="assets/img/CS224N_Project_Poster.png" title="Method" class="img-fluid rounded z-depth-1" %}
</div>​
